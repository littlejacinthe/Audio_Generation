{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gendyn_GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMS00I4fImq9Mft6tUOw0NS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/littlejacinthe/Audio_Generation/blob/main/Gendyn_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HguKxcnE3Xab"
      },
      "source": [
        "Gendyn GAN is a combination of Dynamic Stochastic Synthesis and audio GAN for sound generation. \r\n",
        "\r\n",
        "References:\r\n",
        "\r\n",
        "https://csound.com/docs/manual/gendy.html : Csound documentation for Gendy\r\n",
        "\r\n",
        "https://csound.com/docs/ctcsound/cookbook.html : Csound Python API, ctcsound\r\n",
        "\r\n",
        "https://github.com/chrisdonahue/wavegan : WaveGAN, raw audio GAN model\r\n",
        "\r\n",
        "https://www.kaggle.com/mrhippo/generating-bird-sound-with-simple-gans : GAN application to audio\r\n",
        "\r\n",
        "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html : Pytorch DCGAN tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860CxTml5AMy"
      },
      "source": [
        "First, you'll have to install Csound in your environment.\r\n",
        "\r\n",
        "Instructions to install Csound from : \r\n",
        "[Csound-Github](https://github.com/csound/csound/blob/develop/BUILD.md#fedora)\r\n",
        "\r\n",
        "\r\n",
        "Note: If you don't have an audio device in your environment, just install a virtual one like pulseaudio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFD69nji3MvV"
      },
      "source": [
        "#install dependencies\r\n",
        "! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html \r\n",
        "! pip install torchaudio==0.6.0\r\n",
        "\r\n",
        "# make sure you have the right version of kaggle installed (you can skip this if you already have version 1.5.6)\r\n",
        "! pip uninstall -y kaggle\r\n",
        "! pip install --upgrade pip\r\n",
        "! pip install kaggle==1.5.6\r\n",
        "\r\n",
        "# python library for csound\r\n",
        "!pip install ctcsound"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wft2BCxZ5oDg"
      },
      "source": [
        "import numpy\r\n",
        "from audio import *\r\n",
        "from fastai.basics import *\r\n",
        "import torch\r\n",
        "import ctcsound\r\n",
        "import time\r\n",
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrB8Azko7Ol1"
      },
      "source": [
        "#make sure you're on gpu\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po3vJ9A86E2Q"
      },
      "source": [
        "Next, we need to install [csoundmagics](https://github.com/csound/ctcsound/blob/master/cookbook/05-installingCsoundmagics.ipynb), the extension of Csound for Jupyter. For that we'll need files in the ctcsound repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VnbJbGS50GJ"
      },
      "source": [
        "! git clone https://github.com/csound/ctcsound.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69W8pmGN6Jz1"
      },
      "source": [
        "The following cell indicates us where to move the files we need for csoundmagics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2WYiHDS57EX"
      },
      "source": [
        "import notebook\r\n",
        "import os.path\r\n",
        "import site\r\n",
        "import shutil\r\n",
        "\r\n",
        "# Copy csoundmagics in user site-packages dir\r\n",
        "dest_magics = site.getsitepackages()[0]\r\n",
        "print('Location for csoundmagics.py:\\n%s' % dest_magics)\r\n",
        "#shutil.copy(\"csoundmagics/csoundmagics.py\", dest_magics)\r\n",
        "\r\n",
        "# Copy csound mode in codemirror\r\n",
        "dest_csmode = os.path.join(notebook.DEFAULT_STATIC_FILES_PATH, \"components\", \"codemirror\", \"mode\", \"csound\")\r\n",
        "print('Location for csoundmode.js:\\n%s' % dest_csmode)\r\n",
        "if not os.path.exists(dest_csmode):\r\n",
        "    os.mkdir(dest_csmode)\r\n",
        "#shutil.copy(\"csoundmagics/csound.js\", dest_csmode)\r\n",
        "\r\n",
        "# Copy custom.js in jupyter dir\r\n",
        "dest_custom = os.path.join(notebook.extensions.jupyter_config_dir(), \"custom\")\r\n",
        "print('Location for custom.js:\\n%s' % dest_custom)\r\n",
        "if not os.path.exists(dest_custom):\r\n",
        "    os.mkdir(dest_custom)\r\n",
        "#shutil.copy(\"csoundmagics/custom.js\", dest_custom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uAqjEjw6-Ln"
      },
      "source": [
        "Csound score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxtmOyqr6Mh-"
      },
      "source": [
        "# this definition will create a csound instance and generate a one second long audio file with the Gendy opcode. \r\n",
        "# we will use it when training our deep learning model, instead of the Gaussian distribution used in most generators\r\n",
        "\r\n",
        "def gendy_noise():\r\n",
        "    \r\n",
        "    #reload everytime we call the function to make sure the buffers are empty\r\n",
        "    %reload_ext csoundmagics \r\n",
        "    c = ICsound(port=12894)\r\n",
        "    c.startEngine()\r\n",
        "    # Our Orchestra for our project\r\n",
        "    orc = \"\"\"\r\n",
        "    sr=44100\r\n",
        "    ksmps=32\r\n",
        "    nchnls=2\r\n",
        "    0dbfs=1\r\n",
        "\r\n",
        "    instr 1 \r\n",
        "\r\n",
        "    aout gendy 0.7, 1, 1, 1, 1, 20, 1000, 0.5, 0.5, 40\r\n",
        "    outs aout, aout\r\n",
        "    endin\"\"\"\r\n",
        "\r\n",
        "    #c = ctcsound.Csound()    # create an instance of Csound\r\n",
        "    c.setOption(\"-odac\")  # Set option for Csound\r\n",
        "    c.setOption(\"-m7\")  # Set option for Csound\r\n",
        "    c.compileOrc(orc)     # Compile Orchestra from String\r\n",
        "\r\n",
        "    sco = \"i1 0 1\\n\"\r\n",
        "    \r\n",
        "\r\n",
        "    c.sendScore(sco)\r\n",
        "    c.readScore(sco)     # Read in Score generated from notes \r\n",
        "    \r\n",
        "    c.startRecord('gendyn.wav')\r\n",
        "    c.start()\r\n",
        "    #c.perform()\r\n",
        "    \r\n",
        "    time.sleep(1) #wait one second as our score is one second long\r\n",
        "    c.stopRecord()\r\n",
        "    c.stopEngine()\r\n",
        "    c.reset()\r\n",
        "     \r\n",
        "    #x, sr = librosa.load('gendyn.wav')\r\n",
        "    x, sr = soundfile.read('gendyn.wav') #read the file we just created\r\n",
        "    torch_noise = torch.from_numpy(x).to(device) #transform it into a torch tensor, on gpu\r\n",
        "    size = len(torch_noise) * 2 # multiplying by 2 as the sound file originally has 2 channels (stereo), but we're downmixing to mono / 1 channel\r\n",
        "    pre_shape = int(size) / int(bs) \r\n",
        "    shape = int(pre_shape) #shape needs to be an int\r\n",
        "    new_noise = torch.reshape(torch_noise, ([16, 1, shape])).to(dev, dtype=torch.float) #reshape the tensor so it's ready for the Generator\r\n",
        "        \r\n",
        "    return new_noise, shape\r\n",
        "    del c #delete the csound instance \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-jTVKeY7A2u"
      },
      "source": [
        "noise, shape = gendy_noise() #test if it works"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m88H7G4U7XQl"
      },
      "source": [
        "# the dataset is stored on Kaggle so first let's make a directory for the kaggle key\r\n",
        "!mkdir ~/.kaggle/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMF8zu6Z7aSG"
      },
      "source": [
        "# move the .json file with your kaggle api key to the directory we just made, you have to upload your own !\r\n",
        "! mv kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGVGhOy-7cjd"
      },
      "source": [
        "# DL the dataset \r\n",
        "# This dataset is made of 718 audio files (mp3) from the Free Music Archive website. All files from this dataset have both tags free-jazz and improv. \r\n",
        "# Dataset is around 10GB so make sure you have the disk space required.\r\n",
        "! kaggle datasets download -d jacinthecarlier/improvisation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-JTL5k37fpQ"
      },
      "source": [
        "# unzip the dataset to the appropriate directory\r\n",
        "! unzip /content/improvisation.zip -d /content/jazz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGQENVM27imt"
      },
      "source": [
        "tfms=None #no transforms needed\r\n",
        "data_folder = '/content/jazz/Jazz' #path of the folder containing sound files\r\n",
        "bs = 16 #batch size, adapt if necessary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jwOlVUR7k2z"
      },
      "source": [
        "#Configurate sounds :\r\n",
        "# - experiment different segment_size, the bigger the size the bigger the network, \r\n",
        "# - 16kHz sample rate (lower than the standard 44100Hz because it makes smaller files which is easier to process) --> doesn't work well so use 44100 Hz to get better results\r\n",
        "# - downmix from stereo to mono so the files have one channel only = 1 Dimension\r\n",
        "config_segment = AudioConfig(segment_size = 1000, resample_to=44100, downmix=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahzlbtOG7pBb"
      },
      "source": [
        "db_audio = (AudioList.from_folder(data_folder, config=config_segment) #load them in a list, downmix, resample and sgement \r\n",
        "                .split_none().label_empty() #no split no label, this is not a classification model\r\n",
        "                .transform(tfms=tfms) #no transforms necessary here, we're using the wave samples as is\r\n",
        "                .databunch(bs=bs)) #adapt batch size if necessary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFmkOqrJ7rNx"
      },
      "source": [
        "train_set = db_audio.dl(ds_type = DatasetType.Train) #transforms the fastai audio databunch in a pytorch dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_YMlxsX7tiE"
      },
      "source": [
        "#Generator Model\r\n",
        "ngf = 128 #our input can be divided by 128 so we can use it as reference just like it's done for image models\r\n",
        "nc = 1 #input dim = 1 channel as we are using one dimensional waveforms\r\n",
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "    def __init__(self, ngpu):\r\n",
        "        super(Generator, self).__init__()\r\n",
        "        self.ngpu = ngpu \r\n",
        "        self.main = nn.Sequential(\r\n",
        "            \r\n",
        "            nn.ConvTranspose1d(nc, ngf * 8, 2, 2, 1),  \r\n",
        "            nn.Dropout(p=0.2), #dropout helps stabilize training\r\n",
        "            #nn.BatchNorm1d(ngf * 8), #batchnorm is not recommended for waveform models, and it didn't make the model more efficient during experiments\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            \r\n",
        "            nn.ConvTranspose1d(ngf * 8, ngf * 4, 2, 2, 4),  \r\n",
        "            nn.Dropout(p=0.2),\r\n",
        "           # nn.BatchNorm1d(ngf * 4),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            # \r\n",
        "            nn.ConvTranspose1d(ngf * 4, ngf * 2, 2, 2, 4),\r\n",
        "            nn.Dropout(p=0.2),\r\n",
        "           # nn.BatchNorm1d(ngf * 2),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            #\r\n",
        "            nn.ConvTranspose1d(ngf * 2, ngf, 2, 2, 4),\r\n",
        "            nn.Dropout(p=0.2),\r\n",
        "          #  nn.BatchNorm1d(ngf),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            #\r\n",
        "            nn.ConvTranspose1d(ngf, 1, 2, 2, 4),\r\n",
        "            nn.Tanh(),\r\n",
        "\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return self.main(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeW0yqgD7zHP"
      },
      "source": [
        "ngpu=1\r\n",
        "dev = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\") # getting gpu\r\n",
        "torch.cuda.is_available() #checking if gpu is available \r\n",
        "\r\n",
        "# Create the generator\r\n",
        "netG = Generator(ngpu).to(dev)\r\n",
        "\r\n",
        "# Print the model\r\n",
        "print(netG)\r\n",
        "\r\n",
        "from torchsummary import summary\r\n",
        "summary(netG,(1, 5504))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRDWXF587zsk"
      },
      "source": [
        "ndf = 128\r\n",
        "nc = 1\r\n",
        "\r\n",
        "class Discriminator(nn.Module):\r\n",
        "    def __init__(self, ngpu):\r\n",
        "        super(Discriminator, self).__init__()\r\n",
        "        self.ngpu = ngpu\r\n",
        "        self.main = nn.Sequential(\r\n",
        "            # \r\n",
        "            nn.Conv1d(nc, ndf, 2, 8, 2),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            # \r\n",
        "            nn.Conv1d(ndf, ndf * 2, 2, 8, 2),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            # \r\n",
        "            nn.Conv1d(ndf * 2, ndf * 4, 2, 8, 2),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "\r\n",
        "            nn.Conv1d(ndf * 4, ndf * 8, 2, 8, 2),\r\n",
        "            nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            # \r\n",
        "            nn.Conv1d(ndf * 8, 1, 2, 9, 2),\r\n",
        "            nn.Sigmoid(),\r\n",
        "\r\n",
        "\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return self.main(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnO8I-36gFLG"
      },
      "source": [
        "# Create the Discriminator\r\n",
        "netD = Discriminator(ngpu).to(dev)\r\n",
        "\r\n",
        "# Print the model\r\n",
        "print(netD)\r\n",
        "\r\n",
        "summary(netD,(1,22144)) #22144 = size of input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Ho6HHRgMHG"
      },
      "source": [
        "# Initialize BCELoss function\r\n",
        "criterion = nn.BCELoss()\r\n",
        "\r\n",
        "# Create batch of latent vectors that we will use to visualize\r\n",
        "#  the progression of the generator\r\n",
        "fixed_noise = torch.randn(bs, 1, 22144, device=dev)\r\n",
        "\r\n",
        "# Establish convention for real and fake labels during training\r\n",
        "real_label = 1.\r\n",
        "fake_label = 0.\r\n",
        "\r\n",
        "lr = 0.0002 #learning rate\r\n",
        "\r\n",
        "# Setup Adam optimizers for both G and D\r\n",
        "optimizerD = optim.SGD(netD.parameters(), lr=lr, weight_decay=0.2) \r\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTpuLkqhgQUW"
      },
      "source": [
        "# Training Loop\r\n",
        "\r\n",
        "# Lists to keep track of progress\r\n",
        "samples_list = [] # the loop will generate samples while training and put them in this list\r\n",
        "G_losses = []\r\n",
        "D_losses = []\r\n",
        "iters = 0\r\n",
        "num_epochs = 10\r\n",
        "b_size = 16\r\n",
        "\r\n",
        "print(\"Starting Training Loop...\")\r\n",
        "# For each epoch\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    # For each batch in the dataloader\r\n",
        "    for i, data in enumerate(train_set, 0):\r\n",
        "        \r\n",
        "        ############################\r\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\r\n",
        "        ###########################\r\n",
        "        ## Train with all-real batch\r\n",
        "        netD.zero_grad()\r\n",
        "        # Format batch\r\n",
        "        real_cpu = data[0].to(device)\r\n",
        "        new_cpu = torch.reshape(real_cpu, ([bs, 1, 128 * 173])).to(dev) #fastai_audio prepares data as 2 dimensional so we put it back to 1D\r\n",
        "        b_size = new_cpu.size(0)\r\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=dev) # labelling our dataset as real data\r\n",
        "        # Forward pass real batch through D\r\n",
        "        output = netD(new_cpu).view(-1)\r\n",
        "        # Calculate loss on all-real batch\r\n",
        "        errD_real = criterion(output, label)\r\n",
        "        # Calculate gradients for D in backward pass\r\n",
        "        errD_real.backward()\r\n",
        "        D_x = output.mean().item()\r\n",
        "\r\n",
        "        ## Train with all-fake batch\r\n",
        "        # Generate batch of latent vectors\r\n",
        "        noise, shape, size = gendy_noise()\r\n",
        "        if noise.shape[2] < 1000: #if the gendy_noise function sends an audio file too small it won't go through the model\r\n",
        "            noise, shape, size = gendy_noise() # so let's make sure it does\r\n",
        "        # Generate fake image batch with G\r\n",
        "        fake = netG(new_noise)\r\n",
        "        # Classify all fake batch with D\r\n",
        "        output = netD(fake.detach()).view(-1)\r\n",
        "        label_size = int(len(output))\r\n",
        "        label = torch.full((label_size,), fake_label, dtype=torch.float, device=dev) #labelling data as fake\r\n",
        "        # Calculate D's loss on the all-fake batch\r\n",
        "        errD_fake = criterion(output, label)\r\n",
        "        # Calculate the gradients for this batch\r\n",
        "        errD_fake.backward()\r\n",
        "        D_G_z1 = output.mean().item()\r\n",
        "        # Add the gradients from the all-real and all-fake batches\r\n",
        "        errD = errD_real + errD_fake\r\n",
        "        # Update D\r\n",
        "        optimizerD.step()\r\n",
        "\r\n",
        "        ############################\r\n",
        "        # (2) Update G network: maximize log(D(G(z)))\r\n",
        "        ###########################\r\n",
        "        netG.zero_grad()\r\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\r\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\r\n",
        "        output = netD(fake).view(-1)\r\n",
        "        # Calculate G's loss based on this output\r\n",
        "        errG = criterion(output, label)\r\n",
        "        # Calculate gradients for G\r\n",
        "        errG.backward()\r\n",
        "        D_G_z2 = output.mean().item()\r\n",
        "        # Update G\r\n",
        "        optimizerG.step()\r\n",
        "        \r\n",
        "        # Output training stats\r\n",
        "        if i % 50 == 0:\r\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\r\n",
        "                  % (epoch, num_epochs, i, len(train_set),\r\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\r\n",
        "        \r\n",
        "        # Save Losses for plotting later\r\n",
        "        G_losses.append(errG.item())\r\n",
        "        D_losses.append(errD.item())\r\n",
        "\r\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_set)-1)):\r\n",
        "            with torch.no_grad():\r\n",
        "                fake = netG(fixed_noise) #testing our model on random numbers\r\n",
        "            samples_list.append(fake) #putting generated samples in a list\r\n",
        "\r\n",
        "        \r\n",
        "        iters += 1\r\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMolfOvIhuKT"
      },
      "source": [
        "a = torch.stack(samples_list) #list -> tensors\r\n",
        "\r\n",
        "size = a.shape[0] * a.shape[1] * a.shape[2]\r\n",
        "ready = torch.reshape(a, [1,size]) #reshape to make mono or stereo audio waveform\r\n",
        "\r\n",
        "b = ready.detach().cpu() \r\n",
        "c = b.numpy() #transform from torch tensor to numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq9Xe5TwiMpw"
      },
      "source": [
        "import IPython.display as ipd\r\n",
        "ipd.Audio(c, rate=44100) #read audio from numpy array"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}